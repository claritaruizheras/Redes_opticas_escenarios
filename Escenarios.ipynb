{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escenarios integrados en un mismo script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Librerías necesarias para el desarrollo del script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from custom_env.redes_opticas_env import RedesOpticasEnv as RedesOpticasEnv1\n",
    "from custom_env.redes_opticas_env_2 import RedesOpticasEnv as RedesOpticasEnv2\n",
    "from custom_env.redes_opticas_env_3 import RedesOpticasEnv as RedesOpticasEnv3\n",
    "from custom_env.redes_opticas_env_4 import RedesOpticasEnv as RedesOpticasEnv4\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Declaración de los distintos entornos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_env(num_ont, TxRate=10e9,B_guaranteed=[], n_ciclos=200, rank=0, seed=0):\n",
    "    def _init():\n",
    "        env = RedesOpticasEnv1(render_mode=None,seed=seed, num_ont=num_ont, TxRate=TxRate, B_guaranteed=B_guaranteed,n_ciclos=n_ciclos)\n",
    "        env = Monitor(env, filename=\"./reward_logs/reward\")\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "def make_env_2(num_ont, TxRate=10e9,B_guaranteed=[], n_ciclos=200, rank=0, seed=0):\n",
    "    def _init():\n",
    "        env = RedesOpticasEnv2(render_mode=None,seed=seed, num_ont=num_ont, TxRate=TxRate, B_guaranteed=B_guaranteed,n_ciclos=n_ciclos)\n",
    "        env = Monitor(env, filename=\"./reward_logs/reward\")\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "def make_env_3(num_ont, TxRate=10e9,B_guaranteed=[], n_ciclos=200, rank=0, seed=0):\n",
    "    def _init():\n",
    "        env = RedesOpticasEnv3(render_mode=None,seed=seed, num_ont=num_ont, TxRate=TxRate, B_guaranteed=B_guaranteed,n_ciclos=n_ciclos)\n",
    "        env = Monitor(env, filename=\"./reward_logs/reward\")\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "def make_env_4(num_ont, TxRate=10e9,B_guaranteed=[], n_ciclos=200, rank=0, seed=0):\n",
    "    def _init():\n",
    "        env = RedesOpticasEnv4(render_mode=None,seed=seed, num_ont=num_ont, TxRate=TxRate, B_guaranteed=B_guaranteed,n_ciclos=n_ciclos)\n",
    "        env = Monitor(env, filename=\"./reward_logs/reward\")\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"Selecciona el entorno a inicializar:\")\n",
    "    print(\"1: RedesOpticasEnv1\")\n",
    "    print(\"2: RedesOpticasEnv2\")\n",
    "    print(\"3: RedesOpticasEnv3\")\n",
    "    print(\"4: RedesOpticasEnv4\")\n",
    "    entorno_seleccionado = input(\"Introduce el número de entorno (1, 2, 3 o 4): \")\n",
    "\n",
    "    if entorno_seleccionado == \"1\":\n",
    "        make_env_func = make_env\n",
    "    elif entorno_seleccionado == \"2\":\n",
    "        make_env_func = make_env_2\n",
    "    elif entorno_seleccionado == \"3\":\n",
    "        make_env_func = make_env_3\n",
    "    elif entorno_seleccionado == \"4\":\n",
    "        make_env_func = make_env_4\n",
    "    else:\n",
    "        raise ValueError(\"Selección de entorno no válida. Debe ser 1, 2, 3 o 4.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Definición del agente y entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    if entorno_seleccionado == \"1\" or entorno_seleccionado == \"2\" or entorno_seleccionado == \"3\":\n",
    "            env_id = 'RedesOpticasEnv-v0'  \n",
    "            num_test = 20  \n",
    "            seed = np.random.randint(0, 10)  \n",
    "            num_envs = 1  \n",
    "            num_ont=16\n",
    "            TxRate=10e9\n",
    "            T=0.002\n",
    "            B_available=TxRate*T\n",
    "            B_guaranteed=[600e6]*num_ont\n",
    "            for i in range(num_ont):\n",
    "                B_max[i]=B_guaranteed[i]*T\n",
    "            n_ciclos= 400\n",
    "\n",
    "    elif entorno_seleccionado == \"4\":\n",
    "            \n",
    "            num_test = 20\n",
    "            seed = np.random.randint(0, 10)\n",
    "            num_ont = 16\n",
    "            num_guaranteed_ont = 6\n",
    "            TxRate = 10e9\n",
    "            T = 0.002\n",
    "            B_guaranteed = [600e6 if i < num_guaranteed_ont else 0 for i in range(num_ont)]\n",
    "            B_max = [b * T for b in B_guaranteed]\n",
    "            n_ciclos = 4000\n",
    "\n",
    "            ont_tipo1 = list(range(3))\n",
    "            ont_tipo2 = list(range(3, num_ont))\n",
    "    \n",
    "    vec_env = DummyVecEnv([make_env_func(num_ont,TxRate,B_guaranteed,n_ciclos, rank=i, seed=42) for i in range(num_envs)])\n",
    "    \n",
    "    n_steps = 256 \n",
    "    batch_size = 128 \n",
    "\n",
    "    if entorno_seleccionado == \"1\" or entorno_seleccionado == \"2\" or entorno_seleccionado == \"3\":\n",
    "        model = PPO(\n",
    "                \"MlpPolicy\",\n",
    "                vec_env,\n",
    "                n_steps=n_steps,  \n",
    "                batch_size=batch_size,  \n",
    "                learning_rate =3.593813663804625e-05,\n",
    "                n_epochs= 20,\n",
    "                gamma= 0.9286872177396786,\n",
    "                gae_lambda= 0.9797433034291914,\n",
    "                clip_range= 0.2847644623515798,\n",
    "                clip_range_vf= 0.1988417058668721,\n",
    "                max_grad_norm= 3.1942678576486786,\n",
    "                device=\"cpu\"\n",
    "        )\n",
    "\n",
    "    elif entorno_seleccionado == \"4\":\n",
    "           model = PPO(\n",
    "                \"MlpPolicy\", \n",
    "                vec_env, \n",
    "                learning_rate= 0.001,\n",
    "                n_steps= 256,\n",
    "                batch_size= 32,\n",
    "                n_epochs= 30,\n",
    "                gamma= 0.94,\n",
    "                gae_lambda= 0.8878686405859539,\n",
    "                clip_range= 0.3492601502681344,\n",
    "                clip_range_vf= 0.14642359654938747,\n",
    "                max_grad_norm= 3.460846959513738,\n",
    "                device=\"cpu\"\n",
    "        )\n",
    "\n",
    "    # Registro del tiempo antes del entrenamiento\n",
    "    start_time = time.time()\n",
    "    # Línea de entrenamiento del modelo\n",
    "    model.learn(total_timesteps=100000, progress_bar=True)\n",
    "\n",
    "    # Registro del tiempo después del entrenamiento\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Cálculo del tiempo total de entrenamiento\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"El tiempo de entrenamiento fue de {training_time} segundos.\")\n",
    "\n",
    "    \n",
    "    # Fase de pruebas\n",
    "\n",
    "    #Establecemos un unico episodio a investigar\n",
    "    num_test_episodes = 1  # Número de episodios de prueba\n",
    "\n",
    "    # Lista para guardar la información de cada episodio\n",
    "    episode_info = []  \n",
    "\n",
    "    # Lista de en cada ont guardar el valor de su capacidad, de entrada salida y del pendiente(entrada-salida)\n",
    "    trafico_entrada_onts= []\n",
    "    B_alloc_onts = []\n",
    "    trafico_colas_onts=[]\n",
    "\n",
    "    # Guardar los estados de ON y OFF del estado de pareto\n",
    "    #estados_on_off_recolectados = []\n",
    "\n",
    "    #Capacidad de la OLT\n",
    "    tamano_cola=[]\n",
    "\n",
    "    \n",
    "    obs = vec_env.reset()  # Resetea el entorno al estado inicial\n",
    "    _states = None  # Inicializa el estado del modelo\n",
    "    \n",
    "    for episode in range(num_test_episodes):\n",
    "        \n",
    "        done = np.array([False]*num_envs)  # Inicializa 'done' para todos los entornos\n",
    "        step_counter = 0  # Contador de steps para limitar al numero de ciclos\n",
    "\n",
    "        while step_counter < n_ciclos:\n",
    "            \n",
    "\n",
    "            action, _states = model.predict(obs, state=_states, deterministic=True)  # Usamos el modelo para predecir la acción\n",
    "            obs, rewards, dones, info = vec_env.step(action)\n",
    "\n",
    "            # Guardamos la información del episodio.\n",
    "            episode_info.append(info)\n",
    "            for subinfo in  info:  # Itera sobre cada sub-entorno  \n",
    "                suma = 0\n",
    "                trafico_entrada_onts.append(subinfo['trafico_entrada'])\n",
    "                B_alloc_onts.append(subinfo['B_alloc'])\n",
    "                trafico_colas_onts.append(subinfo['B_demand'])\n",
    "                \n",
    "            done |= dones  # Actualiza 'done' para todos los entornos\n",
    "            step_counter += 1  # Incrementa el contador de steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Fase de test del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Fase de pruebas\n",
    "\n",
    "    #Establecemos un unico episodio a investigar\n",
    "    num_test_episodes = 1  # Número de episodios de prueba\n",
    "\n",
    "    # Lista para guardar la información de cada episodio\n",
    "    episode_info = []  \n",
    "\n",
    "    # Lista de en cada ont guardar el valor de su capacidad, de entrada salida y del pendiente(entrada-salida)\n",
    "    trafico_entrada_onts= []\n",
    "    B_alloc_onts = []\n",
    "    trafico_colas_onts=[]\n",
    "\n",
    "    # Guardar los estados de ON y OFF del estado de pareto\n",
    "    #estados_on_off_recolectados = []\n",
    "\n",
    "    #Capacidad de la OLT\n",
    "    tamano_cola=[]\n",
    "\n",
    "    \n",
    "    obs = vec_env.reset()  # Resetea el entorno al estado inicial\n",
    "    _states = None  # Inicializa el estado del modelo\n",
    "    \n",
    "    for episode in range(num_test_episodes):\n",
    "        \n",
    "        done = np.array([False]*num_envs)  # Inicializa 'done' para todos los entornos\n",
    "        step_counter = 0  # Contador de steps para limitar al numero de ciclos\n",
    "\n",
    "        while step_counter < n_ciclos:\n",
    "\n",
    "            action, _states = model.predict(obs, state=_states, deterministic=True)  # Usamos el modelo para predecir la acción\n",
    "            obs, rewards, dones, info = vec_env.step(action)\n",
    "\n",
    "            # Guardamos la información del episodio.\n",
    "            episode_info.append(info)\n",
    "            for subinfo in  info:  # Itera sobre cada sub-entorno  \n",
    "                suma = 0\n",
    "                trafico_entrada_onts.append(subinfo['trafico_entrada'])\n",
    "                B_alloc_onts.append(subinfo['B_alloc'])\n",
    "                trafico_colas_onts.append(subinfo['B_demand'])\n",
    "                \n",
    "            done |= dones  # Actualiza 'done' para todos los entornos\n",
    "            step_counter += 1  # Incrementa el contador de steps\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Gráficas que describen la asignación del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    def transpuesta(list_ont):\n",
    "        #Hay que hacer la matriz transpuesta de la lista\n",
    "\n",
    "        np_array = np.array(list_ont)\n",
    "        # Transponer el array\n",
    "        transposed_np_array = np_array.T\n",
    "\n",
    "        # Convertir el array transpuesto de NumPy de nuevo a una lista de listas de Python\n",
    "        list_transpuesta = transposed_np_array.tolist()\n",
    "\n",
    "        return list_transpuesta\n",
    "        \n",
    "    plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "    #Hallamos el valor de la lista transpuesta, para los valores de entrada y de salida\n",
    "    trafico_entrada_onts_transpuesta=transpuesta(trafico_entrada_onts)\n",
    "    B_alloc_onts_transpuesta=transpuesta(B_alloc_onts)\n",
    "    trafico_colas_onts_transpuesta=transpuesta(trafico_colas_onts)\n",
    "    \n",
    "    # Convertir list_transpuesta a un array NumPy \n",
    "    array_trafico_entrada_onts= np.array(trafico_entrada_onts_transpuesta)/2000 \n",
    "    array_B_alloc_onts= np.array(B_alloc_onts_transpuesta)/2000\n",
    "    array_pendiente= np.array(trafico_colas_onts_transpuesta)/2000\n",
    "\n",
    "    # Convertir array_transpuesta de vuelta a una lista de listas si es necesario\n",
    "    list_valores_entrada = array_trafico_entrada_onts.tolist()\n",
    "    list_valores_salida = array_B_alloc_onts.tolist()\n",
    "    list_pendiente_fin = array_pendiente.tolist()\n",
    "\n",
    "    maximo_pendiente=[]\n",
    "\n",
    "    #Hallamos el valor maximo de cada ciclo en cada ont para las graficas\n",
    "    for i in range(num_ont):\n",
    "        maximo_pendiente.append(max(list_pendiente_fin[i]))\n",
    "\n",
    "    #Hallar valores de los instantes para el grafico de barras\n",
    "\n",
    "    #Graficas\n",
    "\n",
    "    #Grafica del trafico de entrada y salida de cada O\n",
    "    for i in range(num_ont):\n",
    "\n",
    "        nuevo_x = np.arange(2, 2 * n_ciclos + 1, 2)  # x se duplica para cada punto\n",
    "\n",
    "        #Grafica del trafico de pareto de las redes\n",
    "        plt.figure(figsize=(12, 6))\n",
    "      \n",
    "        plt.xlabel('Tiempo en milisegundos')\n",
    "        plt.ylabel('Ancho de banda en Mbps')\n",
    "        plt.plot(nuevo_x, list_valores_entrada[i], label=f'Tráfico de entrada de la ONT {i+1}')\n",
    "        plt.plot(nuevo_x, list_valores_salida[i], label=f'Tráfico de salida de la ONT {i+1}')\n",
    "        plt.title(f'Grafica del trafico de entrada y salida de la ONT {i+1} en Mbps')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #Grafica del trafico pendiente de la ont determinada\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.xlabel('Tiempo en milisegundos')\n",
    "        plt.ylabel('Tamaño de la cola en Mbits')\n",
    "        plt.plot(nuevo_x,list_pendiente_fin[i], label=f'Grafica del trafico pendiente de la ONT {i+1}')\n",
    "        plt.title(f'Grafica del trafico pendiente de la ONT {i+1} en Mbits en el ciclo determinado')\n",
    "        plt.show()\n",
    "\n",
    "       \n",
    "\n",
    "    df = pd.read_csv(\"./reward_logs/reward.monitor.csv\", skiprows=1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df[\"l\"].cumsum(), df[\"r\"])  # 'l' = length, 'r' = reward\n",
    "    plt.xlabel(\"Paso de tiempo\")\n",
    "    plt.ylabel(\"Recompensa acumulada\")\n",
    "    plt.title(\"Evolución de la recompensa\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    if entorno_seleccionado == \"4\":\n",
    "        \n",
    "        suma_total_B_alloc = np.sum(array_B_alloc_onts, axis=0)  \n",
    "\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        plt.plot(nuevo_x, suma_total_B_alloc, color='tab:purple', label='Suma total B_alloc')\n",
    "        plt.xlabel('Tiempo en milisegundos')\n",
    "        plt.ylabel('Ancho de banda total asignado (Mbps)')\n",
    "        plt.title('Ancho de banda total asignado por ciclo (todas las ONTs)')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()   \n",
    "\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(nuevo_x, jfi_values[:len(nuevo_x)], label=\"JFI (ONTs flexibles)\", color='green')\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.xlabel(\"Tiempo en milisegundos\")\n",
    "        plt.ylabel(\"JFI Index\")\n",
    "        plt.title(\"Evolución del índice de equidad entre ONTs flexibles\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
